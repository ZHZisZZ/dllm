<h1 align="center">dLLM</h1>

<p align="center">
Training Diffusion Large Language Models Made Simple
</p>


## Overview
**dLLM** is an educational library offering clear, unified reference implementations for training **diffusion language models**. It brings much-needed transparency to the entire training and deployment process, making **reproduction and finetuning** of open-weight diffusion language models easier and more accessible. Below are some of the key features that make dLLM special:

- dLLM provides reproduction and finetuning recipes for a variety of open-weight models (e.g., [LLaDA / LLaDA-MoE](https://arxiv.org/abs/2502.09992) and [Dream](https://arxiv.org/abs/2508.15487)), and provides educational reference implementation of training algorithms (e.g., [Edit Flows](https://arxiv.org/abs/2506.09018)).

- dLLM, built on top of [ðŸ¤— Transformers](https://github.com/huggingface/transformers), scales seamlesslyâ€”from edge devices with [LoRA](https://github.com/huggingface/peft) to multi-node clusters with [DeepSpeed](https://github.com/deepspeedai/DeepSpeed) and beyond.

- dLLM provides unified, modular training pipelines (inspired by [ðŸ¤— Transformers Trainer](https://github.com/huggingface/transformers/blob/main/src/transformers/trainer.py)) and well-documented [examples](/examples/), making customization simple and development highly user-friendly.

> [!NOTE]
> This repository is primarily for educational purposes and does not aim for 100% exact reproduction of official models (which is impossible). We hope it serves as a helpful reference for the community â€” contributions and improvements are always welcome!


## Table of Contents
<!-- - [Overview](#overview) -->
- [Features & Documentation](#features--documentation)
- [Setup](#setup)
  <!-- - [Installation](#installation)
  - [(optional) Slurm setup](#optional-slurm-setup) -->
- [Files overview](#files-overview)
- [Roadmap](#roadmap)
- [Citation](#citation)


## Features & Documentation

1. [`examples/llada`](/examples/llada): Finetuning open-weight LLaDA [LLaDA / LLaDA-MoE](https://arxiv.org/abs/2502.09992), as well as reproducing LLaDA by training from scratch on public data (pretraining & finetuning).
2. [`examples/dream`](/examples/dream): Finetuning open-weight Dream [Dream](https://arxiv.org/abs/2508.15487).
3. [`examples/editflow`](/examples/editflow): Educational reference for training [Edit Flow](https://arxiv.org/abs/2506.09018) models, demonstrating how to extend existing DLLMs (e.g., LLaDA and Dream) with *edit operations*â€”insertion, deletion, and substitutionâ€”and how to pretrain or finetune EditFlow models from scratch on public data.
4. More upcoming â€” see [Roadmap](#roadmap).


## Setup
### Installation
```bash
# create and activate conda environment
conda create -n dllm python=3.10 -y
conda activate dllm

# install pytorch with CUDA 11.8 (other pytorch/cuda versions should also work)
pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 \
    --index-url https://download.pytorch.org/whl/cu118

# install requirements
pip install -r requirements.txt

# install dllm package
pip install -e .
```
### (optional) Slurm setup
For slurm users, update [`scripts/train.slurm.sh`](/scripts/train.slurm.sh) for your cluster:
```diff
- #SBATCH --partition=mllm_safety # Note: adjust this for your cluster
- #SBATCH --quotatype=spot        # Note: adjust this for your cluster
+ #SBATCH --partition=YOUR_PARTITION
+ #SBATCH --quotatype=YOUR_QUOTATYPE
```
Next, create a directory for your job logs:
```shell
mkdir logs
```
This folder will store the log files generated by your sbatch jobs, for example `logs/dllm-xxxxx.out` and `logs/dllm-xxxxx.err`.

## Files overview
```
# resuable modules for training / sampling
dllm
â”œâ”€â”€ data
â”œâ”€â”€ __init__.py
â”œâ”€â”€ pipelines
â”‚   â”œâ”€â”€ dream
â”‚   â”œâ”€â”€ editflow
â”‚   â””â”€â”€ llada
â”‚       â”œâ”€â”€ ...
â”‚       â””â”€â”€ trainer.py  # Core training logic
â”œâ”€â”€ tools
â””â”€â”€ utils

# entry points for training / sampling
examples
â”œâ”€â”€ dream
â”œâ”€â”€ editflow
â””â”€â”€ llada
    â”œâ”€â”€ generate.py    # Generation example
    â”œâ”€â”€ pt.py          # Pretraining example
    â”œâ”€â”€ README.md      # Example-level documentations
    â””â”€â”€ sft.py         # SFT example
```

A typical training entry script look like (for example, [`examples/llada/sft.py`](/examples/llada/sft.py)) looks like this:
```python
import transformers

import dllm

model_args, data_args, training_args = parser.parse_args_into_dataclasses()
# ----- Model ------------------------------------------------------------------
model = dllm.utils.get_model(model_args)
# ----- Tokenizer --------------------------------------------------------------
tokenizer = dllm.utils.get_tokenizer(model=model, model_args=model_args)
# ----- Dataset ----------------------------------------------------------------
dataset = "..."

# ----- Training --------------------------------------------------------------
trainer = dllm.pipelines.llada.LLaDATrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
    args=training_args,
    data_collator=transformers.DataCollatorForSeq2Seq(
        tokenizer,
        pad_to_multiple_of=8,
        return_tensors="pt",
        padding=True,
        label_pad_token_id=tokenizer.pad_token_id,  # LLaDA is trained on padding <eos_token>
    ),
)
trainer.train()
```
You can launch the training job locally with `accelerate`, or submit it to a Slurm cluster using `sbatch`.
```shell
# Run locally (DeepSpeed ZeRO-2 with 8 GPUs)
accelerate launch \
    --config_file scripts/accelerate_configs/deepspeed_zero2.yaml \
    examples/llada/sft.py \
    --num_train_epochs 4
```
```shell
# Submit to a Slurm cluster (DeepSpeed ZeRO-2 with 8 GPUs)
sbatch --gres=gpu:8 scripts/train.slurm.sh \
    --accelerate_config "deepspeed_zero2" \
    --script_path "examples/llada/sft.py" \
    --script_args "--num_train_epochs 4"
# Submit to a Slurm cluster (DeepSpeed ZeRO-2 with 2 nodes, 16 GPUs)
sbatch --nodes=2 --gres=gpu:8 scripts/train.slurm.sh \
    --accelerate_config "deepspeed_zero2" \
    --script_path "examples/llada/sft.py" \
    --script_args "--num_train_epochs 4"
```



<!-- ## Quick Start

<details>
<summary>LLaDA / LLaDA-MoE: SFT and Sampling</summary>

### `SFT`
Basic usage of [`LLaDATrainer`](https://github.com/ZHZisZZ/dllm/blob/main/dllm/pipelines/llada/trainer.py#L12). See [`examples/llada/sft.py`](https://github.com/ZHZisZZ/dllm/blob/main/examples/llada/sft.py) for a complete example.
```python
import transformers

import dllm

model_args, data_args, training_args = parser.parse_args_into_dataclasses()
model = dllm.utils.get_model(model_args, training_args)
tokenizer = dllm.utils.get_tokenizer(model_args, model)
dataset = "..."

# ----- Training --------------------------------------------------------------
trainer = dllm.pipelines.llada.LLaDATrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
    args=training_args,
    data_collator=transformers.DataCollatorForSeq2Seq(
        tokenizer, 
        pad_to_multiple_of=8, 
        return_tensors="pt", 
        padding=True,
        label_pad_token_id=tokenizer.pad_token_id, # LLaDA is trained on padding <eos_token>
    )
)
trainer.train()
```

> **Notes (LLaDA-MoE only):**  
> For MoE checkpoints, overwrite `config.json` with the following `model_type` and `auto_map`:  
> ```json
> {
>   "model_type": "lladamoe",
>   "auto_map": {
>     "AutoConfig": "configuration_lladamoe.LLaDAMoEConfig",
>     "AutoModel": "modeling_lladamoe.LLaDAMoEModelLM",
>     "AutoModelForCausalLM": "modeling_lladamoe.LLaDAMoEModelLM",
>   }
> }
> ```


### `Sampling`
See [`examples/llada/generate.py`](https://github.com/ZHZisZZ/dllm/blob/main/examples/llada/generate.py) for a complete example of batch sampling (continuation and infilling).

</details>

<details>
<summary>Dream: SFT and Sampling</summary>

### `SFT`

Basic usage of [`DreamTrainer`](https://github.com/ZHZisZZ/dllm/blob/main/dllm/pipelines/dream/trainer.py#L39). See [`examples/dream/sft.py`](https://github.com/ZHZisZZ/dllm/blob/main/examples/dream/sft.py) for a complete example.
```python
import transformers

import dllm

model_args, data_args, training_args = parser.parse_args_into_dataclasses()
model = dllm.utils.get_model(model_args, training_args)
tokenizer = dllm.utils.get_tokenizer(model_args, model)
dataset = "..."

# ----- Training --------------------------------------------------------------
trainer = dllm.pipelines.dream.DreamTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
    args=training_args,
    data_collator=transformers.DataCollatorForSeq2Seq(
        tokenizer, 
        pad_to_multiple_of=8, 
        return_tensors="pt", 
        padding=True,
        label_pad_token_id=-100 # padding tokens do not count in loss
    )
)
trainer.train()
```


### `Sampling`
See [`examples/dream/generate.py`](https://github.com/ZHZisZZ/dllm/blob/main/examples/dream/generate.py) for a complete example of batch sampling (continuation and infilling).

</details> -->


## Roadmap

- [ ] Support for additional diffusion LLMs.  

- [ ] Support for RL finetuning.


## Citation
```
@misc{dllm,
    author = {Zhanhui Zhou and Lingjie Chen},
    title = {dLLM: Training Diffusion Large Language Models Made Easy},
    howpublished = {https://github.com/ZHZisZZ/dllm},
    note = {Accessed: 2025-10-07},
    year = {2025}
}
```
